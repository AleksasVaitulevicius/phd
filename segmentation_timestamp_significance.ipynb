{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518de7b-3328-4bdf-a2db-9272f2a9e664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "from skfda import FDataGrid\n",
    "from skfda.representation.basis import BSpline\n",
    "from skfda.preprocessing.smoothing import BasisSmoother\n",
    "from skfda.preprocessing.registration import ElasticRegistration, landmark_registration\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skfda.exploratory.depth import IntegratedDepth, ModifiedBandDepth\n",
    "\n",
    "from cycler import cycler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "\tprecision_score, recall_score, f1_score, balanced_accuracy_score, roc_curve, confusion_matrix\n",
    ")\n",
    "\n",
    "\n",
    "agg_columns = ['patient_id', 'slice_id', 'img_type']\n",
    "\n",
    "n_basis=18\n",
    "order=4\n",
    "\n",
    "prc_rm=0.05\n",
    "n_points =111\n",
    "\n",
    "basis = BSpline(domain_range=(0, 1), n_basis=n_basis, order=order)\n",
    "smoother = BasisSmoother(basis=basis, return_basis=True, method='svd')\n",
    "\n",
    "registration = ElasticRegistration()\n",
    "ID = IntegratedDepth()\n",
    "MBD = ModifiedBandDepth()\n",
    "\n",
    "\n",
    "def get_model():\n",
    "\treturn {\n",
    "\t\t'svm': SVC(class_weight='balanced', probability=True),\n",
    "\t\t'rf': RandomForestClassifier(class_weight='balanced', n_jobs=4),\n",
    "\t\t'xgb': xgb.XGBClassifier(tree_method='gpu_hist', eval_metric='logloss', use_label_encoder=False),\n",
    "\t}[MODEL]\n",
    "\n",
    "\n",
    "def cut_ends(bsplined, order=0, prc_rm_start=prc_rm, prc_rm_end=prc_rm, n_points=n_points):\n",
    "\tbsplined_grid = bsplined.derivative(order=order).to_grid(np.linspace(0, 1, n_points))\n",
    "\treturn FDataGrid(\n",
    "\t\tdata_matrix=bsplined_grid.data_matrix[\n",
    "\t\t\t..., int(n_points * prc_rm_start): int(n_points * (1 - prc_rm_end)), 0\n",
    "\t\t],\n",
    "\t\tgrid_points=bsplined_grid.grid_points[0][\n",
    "\t\t\tint(n_points * prc_rm_start): int(n_points * (1 - prc_rm_end))\n",
    "\t\t]\n",
    "\t)\n",
    "\n",
    "\n",
    "def get_landmark_registration(bsplined, order=0):\n",
    "\tbsplined_grid = cut_ends(bsplined, order)\n",
    "\tlandmark_indexes = cut_ends(bsplined, order, prc_rm_end=0.5).data_matrix.argmax(axis=1)\n",
    "\tgrid_points = bsplined_grid.grid_points[0]\n",
    "\tlandmarks = [grid_points[index] for index in np.concatenate(landmark_indexes)]\n",
    "\treturn landmark_registration(bsplined_grid, landmarks)\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred, zero_division=0):\n",
    "\ttn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\tif tn+fp == 0 and zero_division:\n",
    "\t\treturn zero_division\n",
    "\treturn tn / (tn+fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30b897-4887-4d55-a896-aceb990e9d9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529eddf-4fbb-491d-94c4-c009ac7cc86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = '50-slic-zones-features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a4f83-d411-404b-b92c-6bca7c4366c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_n_points = 100\n",
    "\n",
    "\n",
    "def get_descrete_points(fd_smooth):\n",
    "\tt_cut = np.linspace(\n",
    "\t\tstart=0,\n",
    "\t\tstop=fd_smooth.data_matrix.shape[1] - 1,\n",
    "\t\tnum=train_n_points, endpoint=True, dtype=int,\n",
    "\t)\n",
    "\treturn fd_smooth.data_matrix[:, t_cut, 0]\n",
    "\n",
    "\n",
    "segmentations = []\n",
    "\n",
    "for segmentation_id in range(0, 25):\n",
    "# for segmentation_id in [5]:\n",
    "\tprint(segmentation_id)\n",
    "\tfile = f'./segmentations/proportionate-{segmentation_id}.csv'\n",
    "\tdataset = (\n",
    "\tpd.read_csv(\n",
    "\t\tfile,\n",
    "\t\tdtype={\n",
    "\t\t\t'img_type': int,\n",
    "\t\t\t'patient_id': int,\n",
    "\t\t\t'cycle_id': int,\n",
    "\t\t\t'slice_id': int,\n",
    "\t\t\t'label': bool,\n",
    "\t\t\t'mask_int_mean': float,\n",
    "\t\t\t'segment': int,\n",
    "\t\t},\n",
    "\t)\n",
    "\t.drop_duplicates()\n",
    "\t.sort_values(agg_columns + ['cycle_id'])\n",
    "\t)\n",
    "\tdataset = dataset.merge(dataset.query('label').patient_id.drop_duplicates())\n",
    "\t# dataset = dataset[dataset.patient_id.apply(lambda x: x not in [3, 6, 29, 86, 108, 119, 140])]\n",
    "\tdataset = dataset[dataset.patient_id.apply(lambda x: x in [2, 15, 28, 32, 35, 39, 40, 41, 45, 50, 52, 64, 66])]\n",
    "\tts = (\n",
    "\t\tdataset[['patient_id', 'cycle_id']].drop_duplicates()\n",
    "\t\t\t.groupby('patient_id').cycle_id.count()\n",
    "\t\t\t.apply(lambda x: np.linspace(0, 1, int(x)))\n",
    "\t\t\t.reset_index()\n",
    "\t)\n",
    "\n",
    "\tdataset = dataset.groupby(agg_columns + ['label']).mask_int_mean.apply(list).reset_index()\n",
    "\tbsplined = dataset.groupby('patient_id').mask_int_mean.apply(list).reset_index().merge(ts)\n",
    "\tbsplined = bsplined.apply(\n",
    "\t\tlambda x: smoother.fit_transform(\n",
    "\t\t\tFDataGrid(data_matrix=x['mask_int_mean'], grid_points=x['cycle_id'])\n",
    "\t\t),\n",
    "\t\taxis='columns',\n",
    "\t)\n",
    "\tprint('extracting features ...')\n",
    "\tregistered = [get_landmark_registration(fd_smooth, 1) for fd_smooth in bsplined]\n",
    "\tunregistered = [cut_ends(fd_smooth, 1) for fd_smooth in bsplined]\n",
    "\t\n",
    "\tdataset['unregistered_ids'] = np.concatenate(\n",
    "\t\t[ID(fd_smooth).reshape(-1, 1) for fd_smooth in unregistered]\n",
    "\t)\n",
    "\tdataset['unregistered_mbds'] = np.concatenate(\n",
    "\t\t[MBD(fd_smooth).reshape(-1, 1) for fd_smooth in unregistered]\n",
    "\t)\n",
    "\tdataset['unregistered_max_values'] = np.concatenate(\n",
    "\t\t[fd_smooth.data_matrix.max(axis=1).reshape(-1, 1) for fd_smooth in unregistered]\n",
    "\t)\n",
    "\t\n",
    "\tdataset['registered_ids'] = np.concatenate(\n",
    "\t\t[ID(fd_smooth).reshape(-1, 1) for fd_smooth in registered]\n",
    "\t)\n",
    "\tdataset['registered_mbds'] = np.concatenate(\n",
    "\t\t[MBD(fd_smooth).reshape(-1, 1) for fd_smooth in registered]\n",
    "\t)\n",
    "\tdataset['registered_max_values'] = np.concatenate(\n",
    "\t\t[fd_smooth.data_matrix.max(axis=1).reshape(-1, 1) for fd_smooth in registered]\n",
    "\t)\n",
    "\tregistered = [get_descrete_points(fd_smooth) for fd_smooth in registered]\n",
    "\tunregistered = [get_descrete_points(fd_smooth) for fd_smooth in unregistered]\n",
    "\n",
    "\tsegmentations += [\n",
    "\t\tdataset.drop('mask_int_mean', axis='columns')\n",
    "\t\t\t.assign(segmentation_id=segmentation_id)\n",
    "\t\t\t.join(\n",
    "\t\t\t\tpd.DataFrame(\n",
    "\t\t\t\t\tnp.concatenate(registered),\n",
    "\t\t\t\t\tcolumns=[f'registered_discrete_{i}' for i in range(100)],\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t\t.join(\n",
    "\t\t\t\tpd.DataFrame(\n",
    "\t\t\t\t\tnp.concatenate(unregistered),\n",
    "\t\t\t\t\tcolumns=[f'unregistered_discrete_{i}' for i in range(100)],\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t]\n",
    "\n",
    "dataset = pd.concat(segmentations)\n",
    "dataset.to_csv(FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66e1a6-6137-4081-b727-584009a24566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "\n",
    "print(FILE)\n",
    "dataset = pd.read_csv(FILE)\n",
    "\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "\treturn -f1_score(y_true, y_pred > 0.5)\n",
    "\n",
    "\n",
    "os.environ['HYPEROPT_FMIN_SEED'] = \"1\"\n",
    "\n",
    "space = {\n",
    "\t'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "\t'gamma': hp.uniform('gamma', 0, 1),\n",
    "\t'reg_alpha' : hp.uniform('reg_alpha', 0, 1),\n",
    "\t'colsample_bytree' : hp.uniform('colsample_bytree', .4, .8),\n",
    "\t'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "\t'n_estimators': 180,\n",
    "\t'seed': 0\n",
    "}\n",
    "\n",
    "result = []\n",
    "# features = r'^unregistered_(discrete_\\d+|ids|mbds|max_values)'\n",
    "features = r'^unregistered_(discrete_\\d+)'\n",
    "result_file = 'unregistered_discrete'\n",
    "\n",
    "for patient, segmentation in dataset[['patient_id', 'segmentation_id']].drop_duplicates().to_records(index=False):\n",
    "\tprint(f'patient - {patient}, segmentation - {segmentation}')\n",
    "\tdataset_group = dataset.query(f'segmentation_id == {segmentation} and patient_id == {patient}')\n",
    "\ty = dataset_group.label.astype(int).to_numpy()\n",
    "\tX = dataset_group[[name for name in dataset.columns if re.match(features, name)]].to_numpy()\n",
    "\tif patient == 2:\n",
    "\t\tprint(X.shape)\n",
    "\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify=y)\n",
    "\t\n",
    "\tneg_class_weight = (y_train == 1).sum() / len(y_train)\n",
    "\tweights = [neg_class_weight if label == 0 else 1 - neg_class_weight for label in y_train ]\n",
    "\tscaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "\tscaler = scaler.fit(X_train)\n",
    "\tX_train = scaler.transform(X_train)\n",
    "\tX_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\tdef objective(space):\n",
    "\t\tmodel = xgb.XGBClassifier(\n",
    "\t\t\tuse_label_encoder=False,\n",
    "\t\t\teval_metric=f1_loss,\n",
    "\t\t\tearly_stopping_rounds=10,\n",
    "\t\t\tn_estimators=space['n_estimators'],\n",
    "\t\t\tmax_depth=int(space['max_depth']),\n",
    "\t\t\tgamma=space['gamma'],\n",
    "\t\t\treg_alpha=space['reg_alpha'],\n",
    "\t\t\tmin_child_weight=space['min_child_weight'],\n",
    "\t\t\tcolsample_bytree=space['colsample_bytree'],\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tmodel.fit(X_train, y_train, sample_weight=weights, eval_set=[( X_train, y_train), ( X_test, y_test)], verbose=False)\n",
    "\t\tpred = model.predict(X_test)\n",
    "\t\treturn {'loss': -f1_score(y_test, pred, zero_division=0), 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "\ttrials = Trials()\n",
    "\tbest_hyperparams = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=400, trials=trials)\n",
    "\tmodel = xgb.XGBClassifier(\n",
    "\t\tuse_label_encoder=False,\n",
    "\t\teval_metric=f1_loss,\n",
    "\t\tearly_stopping_rounds=10,\n",
    "\t\tn_estimators=space['n_estimators'],\n",
    "\t\tmax_depth=int(best_hyperparams['max_depth']),\n",
    "\t\tgamma=best_hyperparams['gamma'],\n",
    "\t\treg_alpha=best_hyperparams['reg_alpha'],\n",
    "\t\tmin_child_weight=best_hyperparams['min_child_weight'],\n",
    "\t\tcolsample_bytree=best_hyperparams['colsample_bytree'],\n",
    "\t)\n",
    "\n",
    "\tmodel.fit(X_train, y_train, sample_weight=weights, eval_set=[( X_train, y_train), ( X_test, y_test)], verbose=False)\n",
    "\tpred = model.predict(X_test)\n",
    "\tmetrics = {\n",
    "\t\t'patient_id': patient,\n",
    "\t\t'segmentation_id': segmentation,\n",
    "\t\t'precision': precision_score(y_test, pred, zero_division=0),\n",
    "\t\t'recall': recall_score(y_test, pred, zero_division=0),\n",
    "\t\t'f1': f1_score(y_test, pred, zero_division=0),\n",
    "\t\t'balanced_accuracy': balanced_accuracy_score(y_test, pred),\n",
    "\t\t'specificity': specificity(y_test, pred, zero_division=0),\n",
    "\t}\n",
    "\tresult += [{**metrics, **best_hyperparams}]\n",
    "\n",
    "pd.DataFrame(result).to_csv(f'./show/fixed/xgboost/{result_file}_no_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2bd02-4a0c-4e42-a8f4-30ba43ebba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "data_file = f'./show/proportionate/xgboost/{result_file}_no_cv.csv'\n",
    "\n",
    "transformed = (\n",
    "\tpd.read_csv(data_file)\n",
    "\t\t.sort_values(['segmentation_id', 'patient_id'])\n",
    "\t\t.groupby('segmentation_id').balanced_accuracy.apply(list).tolist()\n",
    ")\n",
    "\n",
    "stats.friedmanchisquare(*transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
